<html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Making Your Own Datasets · ml5js</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta property="og:title" content="Making Your Own Datasets · ml5js"/><meta property="og:type" content="website"/><meta property="og:url" content="ml5js.github.io/index.html"/><meta property="og:description" content="So you want to make your own dataset! Here are some things to think about during this process."/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"/><script type="text/javascript" src="/scripts/ml5.min.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.5.16/p5.min.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.5.16/addons/p5.dom.min.js"></script><link rel="stylesheet" href="/css/main.css"/></head><body class="sideNavVisible"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/ml5.png"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li><a href="/docs/getting-started.html" target="_self">API</a></li><li><a href="/docs/datasets.html" target="_self">Datasets</a></li><li><a href="/docs/simple-image-classification-example.html" target="_self">Examples</a></li><li><a href="/en/experiments.html" target="_self">Experiments</a></li><li><a href="/docs/glossary-statistics.html" target="_self">Learn</a></li><li><a href="https://github.com/ml5js" target="_self">Code</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Datasets</span></h2></div><div class="navGroups"><div class="navGroup navGroupActive"><h3>Documentation</h3><ul><li class="navListItem"><a class="navItem" href="/docs/getting-started.html">Getting Started</a></li><li class="navListItem"><a class="navItem" href="/docs/imageclassifier.html">Image Classifier</a></li><li class="navListItem"><a class="navItem" href="/docs/knnImage.html">KNN Image Classifier</a></li><li class="navListItem"><a class="navItem" href="/docs/lstm.html">LSTM</a></li><li class="navListItem"><a class="navItem" href="/docs/style-transfer.html">Style Transfer</a></li><li class="navListItem"><a class="navItem" href="/docs/word2vec.html">Word2Vec</a></li></ul></div><div class="navGroup navGroupActive"><h3>Examples</h3><ul><li class="navListItem"><a class="navItem" href="/docs/simple-image-classification-example.html">Image Classification</a></li><li class="navListItem"><a class="navItem" href="/docs/multiple-image-classification-example.html">Multiple Image Classification</a></li><li class="navListItem"><a class="navItem" href="/docs/video-classification-example.html">Video Classification</a></li><li class="navListItem"><a class="navItem" href="/docs/knn-image-example.html">KNN Image Classification</a></li><li class="navListItem"><a class="navItem" href="/docs/lstm-example.html">LSTM</a></li><li class="navListItem"><a class="navItem" href="/docs/lstm-interactive-example.html">LSTM Interactive</a></li><li class="navListItem"><a class="navItem" href="/docs/style-transfer-image-example.html">Style Transfer</a></li><li class="navListItem"><a class="navItem" href="/docs/style-transfer-webcam-example.html">Style Transfer with Webcam</a></li><li class="navListItem"><a class="navItem" href="/docs/word2vec-example.html">Word2Vec</a></li></ul></div><div class="navGroup navGroupActive"><h3>Datasets</h3><ul><li class="navListItem"><a class="navItem" href="/docs/datasets.html">Datasets</a></li><li class="navListItem navListItemActive"><a class="navItem navItemActive" href="/docs/making-datasets.html">Making Your Own Datasets</a></li></ul></div><div class="navGroup navGroupActive"><h3>Training Models</h3><ul><li class="navListItem"><a class="navItem" href="/docs/training-setup.html">Python Setup</a></li><li class="navListItem"><a class="navItem" href="/docs/training-lstm.html">Training a LSTM</a></li></ul></div></div></section></div><script>
          var toggler = document.getElementById('navToggler');
          var nav = document.getElementById('docsNav');
          toggler.onclick = function() {
            nav.classList.toggle('docsSliderActive');
          };
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1>Making Your Own Datasets</h1></header><article><div><span><p>So you want to make your own dataset! Here are some things to think about during this process.</p>
<h2><a class="anchor" aria-hidden="true" name="introductory-questions"></a><a href="#introductory-questions" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introductory questions</h2>
<p><img src="assets/img/datastork.png" style="margin:0px" /></p>
<h3><a class="anchor" aria-hidden="true" name="where-does-data-come-from"></a><a href="#where-does-data-come-from" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Where does data come from?</h3>
<p>Data can be scraped from all over the internet, or from a specific site (like reddit, Twitter, or Flickr). It can be manually created by yourself, or by third-party data creation services (such as Crowdflower). It can be data you already have. It can be accessed using an API, or it can be generated. And of course, much of the world's data comes from companies.</p>
<h3><a class="anchor" aria-hidden="true" name="where-do-labels-tags-come-from"></a><a href="#where-do-labels-tags-come-from" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Where do labels/tags come from?</h3>
<p>If the data is scraped from somewhere, the label is often the keyword used to search for the data (i.e. 'building' or 'tree'). The location of the data is also sometimes its label (i.e. the subreddit name if scraping from a site like reddit). Data can be manually labeled, or labeled using a third-party data tagging service. It can be labels you already have connected to your data. And of course, many labels and tags come from existing taxonomies.</p>
<h2><a class="anchor" aria-hidden="true" name="data-collection"></a><a href="#data-collection" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data collection</h2>
<h3><a class="anchor" aria-hidden="true" name="responsible-data-collection"></a><a href="#responsible-data-collection" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Responsible data collection</h3>
<p>It is important, when collecting data, to think critically at every stage along the collection process.</p>
<p>Some questions to ask yourself include:</p>
<p>Whose data is this? Would they want you to scrape this? Is it public?
Is there a way to ask permission and/or pay for the data?
Can you collect data with appropriate licensing? Flickr, for instance, has an extensive licensing system, and their API allows you to collect images based on permissions.</p>
<p>You want to be careful of under-representation bias in data collection (also known as <a href="https://en.wikipedia.org/wiki/Sampling_bias">sampling bias</a>. For example, if you're collecting a dataset of the best movies of all time, are you asking only those in your immediate circle? For situations where there are obvious gaps in your data, it's important to see if there's a way to collect more, from varied sources.
Read about some work by Mimi Onuoha along these lines here: <a href="https://github.com/MimiOnuoha/missing-datasets">The Library of Missing Datasets</a>.</p>
<p>The converse of this is collecting too much data. Is it really necessary to your project to ask for gender and age? Could collecting religion, ethnicity, or nationality be dangerous now or down the road? Are you collecting only what is absolutely necessary? Is there something that would be better to leave out?</p>
<h3><a class="anchor" aria-hidden="true" name="tagging-and-crowdsourcing"></a><a href="#tagging-and-crowdsourcing" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tagging and crowdsourcing</h3>
<p>Many datasets are created by making small tasks on a crowdsourcing website (such as Crowdflower or Mechanical Turk) and asking people to do these tasks. Crowdsourcing websites makes it easier to do large amounts of tasks that are hard for computers, but easy for humans - for example, identifying an object in an image, or the emotion that a piece of text evokes.</p>
<p>Not all crowdsourcing sites are equal in terms of responsible data collection. When creating a job, pay attention to whether the site allows for feedback from the contributors (people doing your tasks), and whether the site encourages you to pay a fair price (and more than minimum wage) for your jobs.</p>
<p>If you're creating a dataset in this way, remember that you wouldn't have a dataset without these taggers. Pay them well and value their time and contribution.</p>
<h2><a class="anchor" aria-hidden="true" name="things-to-know-for-medium-specific-datasets"></a><a href="#things-to-know-for-medium-specific-datasets" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Things to know for medium-specific datasets</h2>
<h3><a class="anchor" aria-hidden="true" name="images"></a><a href="#images" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Images</h3>
<p>Images often need to be resized to all be the same dimensions. Sometimes, they will need to be converted to black-and-white images. It depends on the neural net architecture you are using, so check the documentation to know for sure. One tool often used to process images programmatically is <a href="http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html">OpenCV</a>.</p>
<h3><a class="anchor" aria-hidden="true" name="text"></a><a href="#text" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Text</h3>
<p>There are many great resources online for gathering text datasets, such as <a href="https://www.gutenberg.org/">Project Gutenberg</a>, a collection of over fifty thousand free books with appropriate copyrights. Text that is scraped from other sources - such as a website, or your own data from a chat log - likely needs to be cleaned and processed. Processing can include tokenizing (splitting
text into words or sentences), removing punctuation, filtering out stopwords (words that you might not care about/don't want to process), and normalizing (such as making sure all the words have the same capitalization).</p>
<p>For more resources on dealing with text data, check out Dan Shiffman's <a href="http://shiffman.net/a2z/intro/">A2Z course!</a></p>
<h3><a class="anchor" aria-hidden="true" name="music"></a><a href="#music" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Music</h3>
<p>Music is different from other mediums because it can be represented in many ways. One thing to think about when making a music dataset is how you want to represent your music. Possible machine learning music inputs include audio files (such as wav files), <a href="https://en.wikipedia.org/wiki/MIDI">MIDI</a>, <a href="https://en.wikipedia.org/wiki/Spectrogram">spectograms</a>/<a href="https://magenta.tensorflow.org/nsynth">rainbowgrams</a>, sheet music, raw audio, or ABC notation. Your music dataset, then, would be a collection of wav files, midi files, ABC notation files, or spectograms. Each of these types of inputs use different machine learning algorithms - for instance, text-based music notation often works with LSTMs, where spectograms often use algorithms more relevant to images.</p>
<h2><a class="anchor" aria-hidden="true" name="preparing-your-dataset-for-machine-learning"></a><a href="#preparing-your-dataset-for-machine-learning" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Preparing your dataset for machine learning</h2>
<h3><a class="anchor" aria-hidden="true" name="training-test-and-validation-datasets"></a><a href="#training-test-and-validation-datasets" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training, test, and validation datasets</h3>
<p>Once you have your dataset, you generally want to split it into training, test, and validation datasets.</p>
<p>Training datasets are used to train the model. Validation datasets are used to change the parameters of the model, and test datasets are used to test the final performance of the model. Over time, we'll add more resources here to further explain these concepts. For now, you just need to know that you need to split your data into these sets.</p>
<p>There is no hard and fast rule for how to split this, but one suggestion is to take all of your data and put about 80% into your training dataset, and 15-16% into test set, and the remaining 4-5% into your validation set.</p>
<p>It is always important to have a lot of training data. If you don't have too much, you may want to put closer to 90% into your training dataset.</p>
<h2><a class="anchor" aria-hidden="true" name="further-reading"></a><a href="#further-reading" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Further reading</h2>
<p>A walkthrough on creating an entire dataset from scratch can be found at <a href="https://itp.nyu.edu/AI/creating-datasets/">ITP's AI blog</a>.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="datasets.html">← Datasets</a><a class="docs-next button" href="training-setup.html">Python Setup →</a></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div><h5>Docs</h5><a href="/docs/getting-started.html">Getting Started</a><a href="/docs/imagenet.html">API Reference</a><a href="/docs/training-models.html">Training Models</a></div><div><h5>Learning</h5><a href="/docs/tutorials.html">Tutorials</a><a href="/docs/glossary-statistics.html">Glossary</a><a href="/docs/resources.html">Resources</a></div><div><h5>Contribute</h5><a href="/experiments.html">Experiments</a><a href="https://github.com/ITPNYU/ml5">Contributing Guide</a><a class="github-button" href="https://github.com/ITPNYU/ml5" data-icon="octicon-star" data-count-href="https://github.com/ITPNYU/ml5" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://itp.nyu.edu" target="_blank" class="fbOpenSource"><img src="/img/itp_logo.png" alt="Facebook Open Source" width="60" height="45"/></a><section class="copyright">This project is currently being maintained at NYU ITP by a community of teachers, residents and students.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
              var search = docsearch({
                apiKey: '4e9582fa59998b865a9fd98ae8d8a9cc',
                indexName: 'ml5js',
                inputSelector: '#search_input_react'
              });
            </script></body></html>